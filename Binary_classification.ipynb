{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL6J0xtg3ams"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading imdb data with most frequent 10000 words\n",
        "\n",
        "from keras.datasets import imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) # you may take top 10,000 word frequently review of movies\n",
        "#consolidating data for EDA(exploratory data analysis: involves gathering all the relevant data into one place and preparing it for analysis )\n",
        "data = np.concatenate((X_train, X_test), axis=0)\n",
        "label = np.concatenate((y_train, y_test), axis=0)\n"
      ],
      "metadata": {
        "id": "idlchnJq6zUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Review is \",X_train[5]) # series of no converted word to vocabulory associated with index\n",
        "print(\"Review is \",y_train[5]) # 0 indicating a negative review and 1 indicating a positive review."
      ],
      "metadata": {
        "id": "b9fxt_He-8ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=imdb.get_word_index() #The code you provided retrieves the word index for the IMDB dataset\n",
        "print(vocab) "
      ],
      "metadata": {
        "id": "xZFeXON4_kpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data #data is a numpy array that contains all the text data from the IMDB dataset, both the training and testing sets."
      ],
      "metadata": {
        "id": "Q65wLcxoQKr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label #label is a numpy array that contains all the sentiment labels from the IMDB dataset, both the training and testing sets.0 indicates a negative review and 1 indicates a positive review."
      ],
      "metadata": {
        "id": "MRtMVfW_QNQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "lHyV6TYn7Eyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "Y3RVAgfP6789"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "3bXE5bL1QtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test # y_test is 25000, which indicates that it contains 25000 sentiment labels, one for each review in the testing set."
      ],
      "metadata": {
        "id": "ZH3ijDRXQuk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform relevant sequence adding on the data\n",
        "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it contains exactly 1000 numbers. \n",
        "# That means we fill every review that is shorter than 500 with zeros. \n",
        "# We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. \n",
        "# We also transform the targets into floats.\n",
        "# sequences is name of method the review less than 1000 we perform padding overthere\n",
        "\n",
        "def vectorize(sequences, dimension = 10000):\n",
        "  # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results"
      ],
      "metadata": {
        "id": "P_Z7n5pe6_-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we split our data into a training and a testing set. \n",
        "# The training set will contain  reviews and the testing set \n",
        "\n",
        "test_x = data[:10000]\n",
        "test_y = label[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = label[10000:]"
      ],
      "metadata": {
        "id": "6laDpGAeJjJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x"
      ],
      "metadata": {
        "id": "hOJcTUYjJn0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y"
      ],
      "metadata": {
        "id": "7UUjUI1YJrPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "id": "vhIxyrxWJtj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "id": "cgl5dFbDJt2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categories:\", np.unique(label))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "# The hstack() function is used to stack arrays in sequence horizontally (column wise).\n",
        "\n",
        "#>>> import numpy as np\n",
        "#>>> x = np.array((3,5,7))\n",
        "#>>> y = np.array((5,7,9))\n",
        "#>>> np.hstack((x,y))\n",
        "# array([3, 5, 7, 5, 7, 9])\n",
        "\n",
        "\n",
        "# You can see in the output above that the dataset is labeled into two categories, either 0 or 1, \n",
        "# which represents the sentiment of the review. \n",
        "\n",
        "# The whole dataset contains 9998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
      ],
      "metadata": {
        "id": "zMpdzIkW-848"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))\n",
        "\n",
        "# The whole dataset contains 9998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
      ],
      "metadata": {
        "id": "2e2sd-WK_YyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you look at the data you will realize it has been already pre-processed.\n",
        "# All words have been mapped to integers and the integers represent the words sorted by their frequency.\n",
        "# This is very common in text analysis to represent a dataset like this. \n",
        "# So 4 represents the 4th most used word, \n",
        "# 5 the 5th most used word and so on... \n",
        "# The integer 1 is reserved for the start marker,\n",
        "# the integer 2 for an unknown word and 0 for padding.\n",
        "\n",
        "# Let's look at a single training example:\n",
        "\n",
        "print(\"Label:\", label[0])"
      ],
      "metadata": {
        "id": "UGIeB-bD_dDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0])\n"
      ],
      "metadata": {
        "id": "Tx2_Wbqo_g3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's decode the first review\n",
        "# Above you see the first review of the dataset which is labeled as positive (1).\n",
        "# The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. \n",
        "# It replaces every unknown word with a “#”. It does this by using the get_word_index() function.\n",
        "\n",
        "\n",
        "# Retrieves a dict mapping words to their index in the IMDB dataset.\n",
        "index = imdb.get_word_index()\n",
        "# If there is a possibility of multiple keys with the same value, you will need to specify the desired behaviour in this case to lookup more than 2 values\n",
        "# ivd = dict((v, k) for k, v in d.items())\n",
        "# If you want to peek at the reviews yourself and see what people have actually written, you can reverse the process too:\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] ) #The purpose of subtracting 3 from i is to adjust the indice,# to indicate that the index was not found.\n",
        "print(decoded) "
      ],
      "metadata": {
        "id": "rq38Xbjy_jlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "2aChnUXHEDAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_index"
      ],
      "metadata": {
        "id": "YqMI4BpREJAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "id": "1bhmYzVhEHS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding sequence to data\n",
        "# Vectorization is the process of converting textual data into numerical vectors and is a process that is usually applied once the text is cleaned.\n",
        "data = vectorize(data)\n",
        "label = np.array(label).astype(\"float32\")\n",
        "\n",
        "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it contains exactly 1000 numbers. \n",
        "# That means we fill every review that is shorter than 1000 with zeros. \n",
        "# We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. \n",
        "# We also transform the targets into floats."
      ],
      "metadata": {
        "id": "Y7ACaI6ECaU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "75EaYgpSHmNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "id": "s710CMfaHpOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check distribution of data\n",
        "\n",
        "# To create plots for EDA(exploratory data analysis) \n",
        "import seaborn as sns #seaborn is a popular Python visualization library that is built on top of Matplotlib and provides a high-level interface for creating informative and attractive statistical graphics.\n",
        "sns.set(color_codes=True)\n",
        "import matplotlib.pyplot as plt # %matplotlib to display Matplotlib plots inline with the notebook\n",
        "%matplotlib inline "
      ],
      "metadata": {
        "id": "CoHZ4IPDDCjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelDF=pd.DataFrame({'label':label})\n",
        "sns.countplot(x='label', data=labelDF)\n",
        "\n",
        "# For below analysis it is clear that data has equel distribution of sentiments.This will help us building a good model."
      ],
      "metadata": {
        "id": "WD3bOTy6DGO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating train and test data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20, random_state=1)"
      ],
      "metadata": {
        "id": "d3VNm4V0DJ2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "ZXKlV64fEZSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "JS032rQMEcYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create sequential model,In deep learning, a Sequential model is a linear stack of layers, where you can simply add layers one after the other\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "85VbeXfBEgca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "# Note that we set the input-shape to 10,000 at the input-layer because our reviews are 10,000 integers long.\n",
        "# The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "# Please note you should always use a dropout rate between 20% and 50%. # here in our case 0.3 means 30% dropout we are using dropout to prevent overfitting. \n",
        "# By the way, if you want you can build a sentiment analysis without LSTMs(Long Short-Term Memory networks), then you simply need to replace it by a flatten layer:\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\")) #ReLU\" stands for Rectified Linear Unit, and it is a commonly used activation function in neural networks. \n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\")) #adds another Dense layer to the model, but with a single neuron instead of 50,i.e. out put layer,it produces the output predictions of the model.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Phrv18kKEo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For early stopping \n",
        "# Stop training when a monitored metric has stopped improving.\n",
        "# monitor: Quantity to be monitored.\n",
        "# patience: Number of epochs with no improvement after which training will be stopped.\n",
        "import tensorflow as tf #TensorFlow provides a wide range of tools and features for data processing, model building, model training, and model evaluation.\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "metadata": {
        "id": "yXAKnIRaE7KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the “adam” optimizer, an algorithm that changes the weights and biases during training.\n",
        "# During training, the weights and biases of a machine learning model are updated iteratively to minimize the difference between the model's predictions and the actual outputs.\n",
        "# We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric.\n",
        "\n",
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "NKv2l42xHKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we're able to train our model. We'll do this with a batch_size of 500 and only for two epochs because I recognized that the model overfits if we train it longer.\n",
        "# batch size defines the number of samples that will be propagated through the network.\n",
        "# For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. \n",
        "# The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \n",
        "# Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \n",
        "# We can keep doing this procedure until we have propagated all samples through of the network. \n",
        "# Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder.\n",
        "# The simplest solution is just to get the final 50 samples and train the network.\n",
        "#The goal is to find the number of epochs that results in good performance on a validation dataset without overfitting to the training data.\n",
        "results = model.fit(X_train, y_train,epochs= 2,batch_size = 500,validation_data = (X_test, y_test),callbacks=[callback]) "
      ],
      "metadata": {
        "id": "RckYtYiNHOw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check mean accuracy of our model\n",
        "print(np.mean(results.history[\"val_accuracy\"])) # Good model should have a mean accuracy significantly higher than 50%"
      ],
      "metadata": {
        "id": "94oaobMeHQ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's plot training history of our model\n",
        "\n",
        "# list all data in history\n",
        "print(results.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(results.history['accuracy']) #Plots the training accuracy of the model at each epoch.\n",
        "plt.plot(results.history['val_accuracy']) #Plots the validation accuracy of the model at each epoch.\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gXT3sRG8HYHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "Pdn0QM_DT_Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Out put analysis,\n",
        "#[0.9865479] is a single prediction value for a particular input sample in the test data. \n",
        "#It is the predicted probability of the positive sentiment class (class 1) for that input.\n",
        "#Since the output activation function of the last layer of the model is sigmoid, which maps the predicted values to a range of [0,1]\n",
        "#,the output values represent the probabilities of the positive class. \n",
        "#In this case, the probability of the positive class for the given input sample is 0.9865479, which is very close to 1, indicating that the model is highly confident in predicting the positive class for that input.\n",
        "\n"
      ],
      "metadata": {
        "id": "pLjG8bZoWX4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
